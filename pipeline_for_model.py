# -*- coding: utf-8 -*-
"""Pipeline_For_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QEyX82Jm4xwSVGeY5G0Tcyb-qvxVUSU8
"""

!pip install kfp --upgrade
!pip install Docker

from kfp import local, dsl
import kfp.components as compile
import kfp.dsl as dsl

import pandas as pd
import os
import requests

local.init(local.DockerRunner())

@dsl.component(
    base_image = 'python:3.12',
    output_component_file='download_dataset_component.yaml')
def download_dataset(url: str, dataset: dsl.Output[dsl.Dataset]) :
    import pandas as pd
    import os

    # Create the directory if it doesn't exist
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # Download the dataset
    df = pd.read_csv(url)

    # Save the dataset locally
    df.to_csv(dataset.path, index=False)
    print(f"Data saved at {dataset.path}")

    # Return the DataFrame
    return dataset.path

@dsl.component(base_image = 'python:3.12', output_component_file = 'preprocess_dataset_component.yaml')
def preprocess_dataset(dataset: dsl.Input[dsl.Dataset], preprocessed_dataset: dsl.Output[dsl.Dataset]):
  import pandas as pd
  dataset.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']
  dataset.to_csv(preprocessed_dataset.path, index = False)
  return preprocessed_dataset.path

@dsl.component(
    base_image = 'python:3.12',
    output_component_file = 'train_test_split.yaml')
def train_test_split(
    dataframe: dsl.Input[dsl.Dataset],
    X_train: dsl.Output[dsl.Dataset],
    X_test : dsl.Output[dsl.Dataset],
    y_train: dsl.Output[dsl.Dataset],
    y_test: dsl.Output[dsl.Dataset]
    ):
  import pandas as pd
  import numpy as np
  from sklearn.model_selection import train_test_split
  import os

  df = pd.read_csv(dataframe.path)


  target_column = 'class'

  X = df.loc[:, df.columns != target_column]
  y = df.loc[:, df.columns == target_column]

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=47)

  np.save(X_train.path, X_train)
  np.save(X_test.path, X_test)
  np.save(y_train.path, y_train)
  np.save(y_test.path, y_test)

@dsl.component(
    base_image = 'python: 3.12',
    output_component_file= 'train_model.yaml'
)
def train_model(
    X_train: dsl.Input[dsl.Dataset],
    y_train: dsl.Input[dsl.Dataset],
    model: dsl.Output[dsl.Model]
):
  import numpy as np
  import pickle
  from sklearn.linear_model import LogisticRegression
  import os

  X_train = np.load(path['X_train'], allow_pickle = True)
  y_train = np.load(path['y_train'], allow_pickle = True)

  classifier = LogisticRegression(max_iter=500)
  classifier.fit(X_train, y_train)

  os.makedirs(os.path.dirname(model_output_path), exist_ok = True)
  with open(model.path, 'wb') as f:
    pickle.dump(classifier, f)
  print("Model Training Completed")

@dsl.pipeline
def mlops_pipeline(url : str, output_path: str):
  download_dataset_task = download_dataset(url = url)
  preprocess_dataset_task = preprocess_dataset(dataset = download_dataset_task.outputs["dataset"])
  train_test_split_task = train_test_split(dataframe = preprocess_dataset_task.outputs['preprocessed_dataset'] )
  train_model_task = train_model(X_train = train_test_split_task.outputs['X_train'],
                                 y_train = train_test_split_task.outputs['y_train'])

if __name__ == '__main__':
  dataset_url = 'https://raw.githubusercontent.com/gbhasin0828/Kubeflow_Files/main/iris.csv'
  output_path = 'data/final_df.csv'
  model_output_path = 'data/model.pkl'

# Compile the pipeline
from kfp.v2 import compiler

pipeline_file = 'mlops_complete_pipeline_file.yaml'

compiler.Compiler().compile(
    pipeline_func = mlops_pipeline,
    package_path= pipeline_file
)