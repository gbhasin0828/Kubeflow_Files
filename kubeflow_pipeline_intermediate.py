# -*- coding: utf-8 -*-
"""KubeFlow_Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19_0dWxpGnnSyPMuxnk1bn8u74vZiKQBN
"""

pip install kfp --upgrade

import kfp

import kfp.components as compile
import kfp.dsl as dsl

from kfp.v2.dsl import component

@component(
    base_image='python:3.12',
    packages_to_install=['pandas', 'numpy', 'scikit-learn']
)

def prepare_data(output_path: str):
    # Save to the mounted volume
    import pandas as pd
    import os

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df = pd.read_csv("https://raw.githubusercontent.com/TripathiAshutosh/dataset/main/iris.csv")
    df.to_csv(output_path, index=False)
    print(f"Data saved at {output_path}")

@component(
    base_image='python:3.12',
    packages_to_install=['pandas', 'numpy', 'scikit-learn']
)

def train_test_split(input_path: str, output_dir: str) -> dict:
    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    import os

    os.makedirs(output_dir, exist_ok=True)
    final_data = pd.read_csv(input_path)

    target_column = 'class'

    X = final_data.loc[:, final_data.columns != target_column]
    y = final_data.loc[:, final_data.columns == target_column]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=47)

    X_train_path = f'{output_dir}/X_train.npy'
    X_test_path = f'{output_dir}/X_test.npy'
    y_train_path = f'{output_dir}/y_train.npy'
    y_test_path = f'{output_dir}/y_test.npy'

    np.save(X_train_path, X_train)
    np.save(X_test_path, X_test)
    np.save(y_train_path, y_train)
    np.save(y_test_path, y_test)

    return {
        "X_train": X_train_path,
        "X_test": X_test_path,
        "y_train": y_train_path,
        "y_test": y_test_path
    }

@component(
    base_image='python:3.12',
    packages_to_install=['pandas', 'numpy', 'scikit-learn']
)


def training_basic_classifier(X_train_path: str, y_train_path: str, model_output_path: str):
    import numpy as np
    import pickle
    from sklearn.linear_model import LogisticRegression
    import os

    X_train = np.load(X_train_path, allow_pickle=True)
    y_train = np.load(y_train_path, allow_pickle=True)

    classifier = LogisticRegression(max_iter=500)
    classifier.fit(X_train, y_train)

    os.makedirs(os.path.dirname(model_output_path), exist_ok=True)
    with open(model_output_path, 'wb') as f:
        pickle.dump(classifier, f)
    print("Model Training Completed")

@component(
    base_image='python:3.12',
    packages_to_install=['pandas', 'numpy', 'scikit-learn']
)

def predict_on_test_data(model_path: str, X_test_path: str, y_pred_path: str):
    import numpy as np
    import pickle

    with open(model_path, 'rb') as f:
        model = pickle.load(f)

    X_test = np.load(X_test_path, allow_pickle=True)
    y_pred = model.predict(X_test)
    np.save(y_pred_path, y_pred)
    print("Predictions saved!")

"""# **Kubeflow Pipeline Work Starts**"""

from kfp.v2.dsl import pipeline
from kfp.v2 import compiler

@pipeline(
    name='prepare_data_pipeline',
    description='A pipeline to prepare data and save it to a CSV file'
)

def create_step_prepare_data(output_path: str = 'data/final_df.csv'):
    # Step 1: Call the prepare_data component
    prepare_data_op = prepare_data(output_path=output_path)

# Compile the pipeline
compiler.Compiler().compile(
    pipeline_func=create_step_prepare_data,
    package_path='prepare_data_pipeline.yaml'
)

@pipeline(
    name='train_test_split',
    description='A pipeline to prepare data for training & testing'
)

def create_step_train_test_split(input_path: str = 'data/final_df.csv', output_dir: str = 'data'):
    """
    A pipeline step that splits the data into training and testing sets.

    Args:
        input_path (str): Path to the input dataset.
        output_dir (str): Directory where the split data will be saved.
    """
    # Call the train_test_split component
    train_test_split_op = train_test_split(
        input_path=input_path,
        output_dir=output_dir
    )

# Compile the pipeline
compiler.Compiler().compile(
    pipeline_func=create_step_train_test_split,
    package_path='train_test_split_pipeline.yaml'
)

from kfp.v2.dsl import pipeline

@pipeline(
    name='training_basic_classifier',
    description='A pipeline to prepare data classifier for training'
)
def create_step_training_basic_classifier(X_train_path: str, y_train_path: str, model_output_path: str = 'data/model.pkl'):
    """
    Pipeline step to train a basic classifier.

    Args:
        X_train_path (str): Path to the training features file.
        y_train_path (str): Path to the training labels file.
        model_output_path (str): Path to save the trained model.
    """
    # Call the training_basic_classifier component
    training_basic_classifier_op = training_basic_classifier(
        X_train_path=X_train_path,
        y_train_path=y_train_path,
        model_output_path=model_output_path
    )

# Compile the pipeline
compiler.Compiler().compile(
    pipeline_func=create_step_training_basic_classifier,
    package_path='training_basic_classifier_pipeline.yaml'
)

from kfp.v2.dsl import pipeline

@pipeline(
    name='predict_on_test_data',
    description='A pipeline to perform predictions'
)

def create_step_predict_on_test_data(model_path: str, X_test_path: str, y_pred_path: str = 'data/y_pred.npy'):
    """
    Pipeline step to perform predictions on test data.

    Args:
        model_path (str): Path to the trained model file.
        X_test_path (str): Path to the test features file.
        y_pred_path (str): Path to save the predictions.
    """
    # Call the predict_on_test_data component
    predict_on_test_data_op = predict_on_test_data(
        model_path=model_path,
        X_test_path=X_test_path,
        y_pred_path=y_pred_path
    )

# Compile the pipeline
compiler.Compiler().compile(
    pipeline_func=create_step_predict_on_test_data,
    package_path='predict_on_test_data_pipeline.yaml'
)

from kfp.v2.dsl import pipeline, component
import os

@pipeline(
    name='iris-classifier-pipeline',
    description='Pipeline for IRIS classification using a shared volume'
)
def iris_classifier_pipeline(data_path: str = '/mnt/data/final_df.csv'):
    """
    Defines the IRIS classification pipeline using a shared persistent volume.

    Args:
        data_path (str): Path to save the prepared dataset in the shared volume.
    """
    # Step 1: Prepare Data
    prepare_data_task = prepare_data(
        output_path=data_path
    ).set_volume_mounts([
        kfp.dsl.VolumeMount(
            name='iris-data-pvc',  # Name of the PVC created earlier
            mount_path='/mnt/data'
        )
    ])

    # Step 2: Train-Test Split
    split_data_task = train_test_split(
        input_path=data_path,
        output_dir='/mnt/data/splits'
    ).set_volume_mounts([
        kfp.dsl.VolumeMount(
            name='iris-data-pvc',
            mount_path='/mnt/data'
        )
    ])

    # Step 3: Train Classifier
    train_model_task = training_basic_classifier(
        X_train_path='/mnt/data/splits/X_train.npy',
        y_train_path='/mnt/data/splits/y_train.npy',
        model_output_path='/mnt/data/model.pkl'
    ).set_volume_mounts([
        kfp.dsl.VolumeMount(
            name='iris-data-pvc',
            mount_path='/mnt/data'
        )
    ])

    # Step 4: Predict on Test Data
    predict_task = predict_on_test_data(
        model_path='/mnt/data/model.pkl',
        X_test_path='/mnt/data/splits/X_test.npy',
        y_pred_path='/mnt/data/y_pred.npy'
    ).set_volume_mounts([
        kfp.dsl.VolumeMount(
            name='iris-data-pvc',
            mount_path='/mnt/data'
        )
    ])